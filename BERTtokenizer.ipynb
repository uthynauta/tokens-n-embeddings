{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import warnings\n",
    "from transformers import BertTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4c15e",
   "metadata": {},
   "source": [
    "# SentencePiece\n",
    "\n",
    "Downloading and creating a dictionary for the standard SentencePiece tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5033fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model has been written to sentencepiece.model\n"
     ]
    }
   ],
   "source": [
    "# URL of the SentencePiece model file\n",
    "url_sp = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
    "\n",
    "# Path to save the downloaded SentencePiece model file\n",
    "sp_model_path = \"sentencepiece.model\"\n",
    "\n",
    "# Download the SentencePiece model file\n",
    "r_sp = requests.get(url_sp)\n",
    "with open(sp_model_path, 'wb') as f:\n",
    "    f.write(r_sp.content)\n",
    "\n",
    "print(f\"SentencePiece model has been written to {sp_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0250ef4",
   "metadata": {},
   "source": [
    "# BertTokenizer\n",
    "\n",
    "Downloading and creating a dictionary for the standard SentencePiece tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3616c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert vocabulary has been written to bert_vocabulary.txt\n"
     ]
    }
   ],
   "source": [
    "url_bert = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
    "r_bert = requests.get(url_bert)\n",
    "filepath_bert = \"bert_vocabulary.txt\"\n",
    "open(filepath_bert, 'wb').write(r_bert.content)\n",
    "print(f\"Bert vocabulary has been written to {filepath_bert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb4f0f",
   "metadata": {},
   "source": [
    "### Comparing BertTokenizer and Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b004d609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens 1: [7011, 78, 86, 71, 39, 5499, 601, 2294, 13, 99, 412, 15, 7010]\n",
      "Tokens 2: [7011, 71, 1327, 227, 80, 1512, 138, 99, 2650, 2310, 15, 7010]\n",
      "Tokens 1 as words: ['[CLS]', 'we', 'are', 'the', 'c', '##ham', '##p', '##ions', ',', 'my', 'friends', '.', '[SEP]']\n",
      "Tokens 2 as words: ['[CLS]', 'the', 'bank', '##er', 'is', 'check', '##ing', 'my', 'credit', 'application', '.', '[SEP]']\n",
      "Detokenized 1: [CLS] we are the champions, my friends. [SEP]\n",
      "Detokenized 2: [CLS] the banker is checking my credit application. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the vocabulary file\n",
    "bert_tokenizer = BertTokenizer(vocab_file=filepath_bert, do_lower_case=True)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokens_1 = bert_tokenizer.encode(\"We are the champions, my friends.\", add_special_tokens=True)\n",
    "tokens_2 = bert_tokenizer.encode(\"The banker is checking my credit application.\", add_special_tokens=True)\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens 1:\", tokens_1)\n",
    "print(\"Tokens 2:\", tokens_2)\n",
    "\n",
    "# Convert tokens back to words\n",
    "tokens_1_words = bert_tokenizer.convert_ids_to_tokens(tokens_1)\n",
    "tokens_2_words = bert_tokenizer.convert_ids_to_tokens(tokens_2)\n",
    "\n",
    "# Print the tokens as words\n",
    "print(\"Tokens 1 as words:\", tokens_1_words)\n",
    "print(\"Tokens 2 as words:\", tokens_2_words)\n",
    "\n",
    "# Detokenize the tokens back to sentences\n",
    "detokenized_1 = bert_tokenizer.decode(tokens_1, skip_special_tokens=False)\n",
    "detokenized_2 = bert_tokenizer.decode(tokens_2, skip_special_tokens=False)\n",
    "\n",
    "# Print the detokenized sentences\n",
    "print(\"Detokenized 1:\", detokenized_1)\n",
    "print(\"Detokenized 2:\", detokenized_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32889371-7efb-49b3-a13e-ae5d300219ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens 1: ['<s>', '▁We', '▁are', '▁the', '▁', 'ch', 'amp', 'ion', 's', ',', '▁my', '▁friend', 's', '.', '</s>']\n",
      "Tokens 2: ['<s>', '▁The', '▁b', 'an', 'k', 'er', '▁is', '▁c', 'he', 'c', 'k', 'ing', '▁my', '▁credit', '▁a', 'p', 'p', 'l', 'ic', 'ation', '.', '</s>']\n",
      "Total number of tokens in the SentencePiece vocabulary: 1000\n",
      "Detokenized 1: <s> ▁We ▁are ▁the ▁ ch amp ion s , ▁my ▁friend s . </s>\n",
      "Detokenized 2: <s> ▁The ▁b an k er ▁is ▁c he c k ing ▁my ▁credit ▁a p p l ic ation . </s>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SentencePiece tokenizer\n",
    "sp_tokenizer = tf_text.SentencepieceTokenizer(model=tf.io.gfile.GFile(sp_model_path, 'rb').read(), out_type=tf.string)\n",
    "\n",
    "# Load the SentencePiece model using sentencepiece library to get special tokens\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "# Get the special tokens\n",
    "cls_token = sp.id_to_piece(1)  # <s>\n",
    "sep_token = sp.id_to_piece(2)  # </s>\n",
    "\n",
    "# Tokenize the sentences\n",
    "sp_tokens_1 = sp_tokenizer.tokenize([\"We are the champions, my friends.\"])\n",
    "sp_tokens_2 = sp_tokenizer.tokenize([\"The banker is checking my credit application.\"])\n",
    "\n",
    "# Add special tokens to the tokenized output\n",
    "sp_tokens_1 = tf.concat([[cls_token], sp_tokens_1.flat_values, [sep_token]], axis=0)\n",
    "sp_tokens_2 = tf.concat([[cls_token], sp_tokens_2.flat_values, [sep_token]], axis=0)\n",
    "\n",
    "# Decode the tokens to UTF-8 strings for readability\n",
    "sp_tokens_1 = [token.decode('utf-8') for token in sp_tokens_1.numpy()]\n",
    "sp_tokens_2 = [token.decode('utf-8') for token in sp_tokens_2.numpy()]\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens 1:\", sp_tokens_1)\n",
    "print(\"Tokens 2:\", sp_tokens_2)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Total number of tokens in the SentencePiece vocabulary: {vocab_size}\")\n",
    "\n",
    "# Detokenize the tokens back to words\n",
    "detokenized_1 = tf.strings.reduce_join(sp_tokens_1, separator=' ', axis=-1)\n",
    "detokenized_2 = tf.strings.reduce_join(sp_tokens_2, separator=' ', axis=-1)\n",
    "\n",
    "# Decode the detokenized sentences to UTF-8 strings for readability\n",
    "detokenized_1 = detokenized_1.numpy().decode('utf-8')\n",
    "detokenized_2 = detokenized_2.numpy().decode('utf-8')\n",
    "\n",
    "# Print the detokenized sentences\n",
    "print(\"Detokenized 1:\", detokenized_1)\n",
    "print(\"Detokenized 2:\", detokenized_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1cc62b0-c5dd-454b-87e1-2d9d6fd34d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokens 1: [7011, 71, 1603, 80, 437, 210, 71, 54, 3715, 1548, 1966, 15, 7010]\n",
      "BERT Tokens 2: [7011, 45, 127, 106, 73, 1603, 85, 71, 49, 899, 1224, 5502, 269, 15, 7010]\n",
      "SentencePiece Tokens 1: [[69, 605, 47, 589, 245, 7, 390, 128, 131, 218, 6]]\n",
      "SentencePiece Tokens 2: [[9, 67, 110, 10, 605, 42, 7, 463, 19, 95, 112, 6]]\n",
      "BERT Tokens 1 as words: ['[CLS]', 'the', 'train', 'is', 'coming', 'down', 'the', 'r', '##ail', '##ro', '##ad', '.', '[SEP]']\n",
      "BERT Tokens 2 as words: ['[CLS]', 'i', 'would', 'like', 'to', 'train', 'for', 'the', 'm', '##ar', '##at', '##ho', '##n', '.', '[SEP]']\n",
      "SentencePiece Tokens 1 as words: ['▁The', '▁train', '▁is', '▁coming', '▁down', '▁the', '▁ra', 'il', 'ro', 'ad', '.']\n",
      "SentencePiece Tokens 2 as words: ['▁I', '▁would', '▁like', '▁to', '▁train', '▁for', '▁the', '▁mar', 'a', 'th', 'on', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Sentences\n",
    "sentences = [\"The train is coming down the railroad.\", \"I would like to train for the marathon.\"]\n",
    "\n",
    "# BERT Tokenization\n",
    "bert_tokens_1 = bert_tokenizer.encode(sentences[0], add_special_tokens=True)\n",
    "bert_tokens_2 = bert_tokenizer.encode(sentences[1], add_special_tokens=True)\n",
    "\n",
    "# SentencePiece Tokenization\n",
    "sp_tokens_1 = sp_tokenizer.tokenize([sentences[0]])\n",
    "sp_tokens_2 = sp_tokenizer.tokenize([sentences[1]])\n",
    "\n",
    "# Print the token values (numbers)\n",
    "print(\"BERT Tokens 1:\", bert_tokens_1)\n",
    "print(\"BERT Tokens 2:\", bert_tokens_2)\n",
    "print(\"SentencePiece Tokens 1:\", sp_tokens_1.numpy().tolist())\n",
    "print(\"SentencePiece Tokens 2:\", sp_tokens_2.numpy().tolist())\n",
    "\n",
    "# Print the Tokens as Words\n",
    "bert_tokens_1_words = bert_tokenizer.convert_ids_to_tokens(bert_tokens_1)\n",
    "bert_tokens_2_words = bert_tokenizer.convert_ids_to_tokens(bert_tokens_2)\n",
    "\n",
    "sp_tokens_1_words = [sp.id_to_piece(token) for token in sp_tokens_1.numpy().tolist()[0]]\n",
    "sp_tokens_2_words = [sp.id_to_piece(token) for token in sp_tokens_2.numpy().tolist()[0]]\n",
    "\n",
    "print(\"BERT Tokens 1 as words:\", bert_tokens_1_words)\n",
    "print(\"BERT Tokens 2 as words:\", bert_tokens_2_words)\n",
    "print(\"SentencePiece Tokens 1 as words:\", sp_tokens_1_words)\n",
    "print(\"SentencePiece Tokens 2 as words:\", sp_tokens_2_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46537df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
